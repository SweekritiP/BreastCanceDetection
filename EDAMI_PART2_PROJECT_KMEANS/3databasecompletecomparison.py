# -*- coding: utf-8 -*-
"""3databasecompletecomparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YavOsEWJLmWSTEiY_dc4p61P2KSIG9Nc

Implimentattion of Kmeans

1.Objective of the Project: This project is to design to use the impliment and determine the accuracy Custom-kmeans(it is kmeans only to determine defined with traditional mathods without the help of any library).


2.To properly test our code and its accuracy i am comparing the results of 3 data sets "White wine" , "Raisin" and " Dry beans" and Using silhouette index and confusion matrix the scores of custom kmeans are compared with results of the Kmeans using sikit learn.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests

# Define the CustomKMeans class
class CustomKMeans:
    def __init__(self, n_clusters, max_iter=300):
        self.n_clusters = n_clusters
        self.max_iter = max_iter

    def fit(self, X):
        # Initialize centroids randomly
        self.centroids = self._initialize_centroids(X)

        for _ in range(self.max_iter):
            # Assign each data point to the nearest centroid
            labels = self._assign_clusters(X)

            # Update centroids based on the mean of data points in each cluster
            new_centroids = self._update_centroids(X, labels)

            # Check for convergence
            if np.allclose(self.centroids, new_centroids):
                break

            self.centroids = new_centroids

        return labels

    def _initialize_centroids(self, X):
        return X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]

    def _assign_clusters(self, X):
        distances = np.sqrt(np.sum((X[:, np.newaxis] - self.centroids) ** 2, axis=2))
        return np.argmin(distances, axis=1)

    def _update_centroids(self, X, labels):
        new_centroids = np.zeros_like(self.centroids)

        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                new_centroids[i] = np.mean(cluster_points, axis=0)
            else:
                new_centroids[i] = X[np.random.choice(len(X))]

        return new_centroids

    def compute_wcss(self, X, labels):
        wcss = 0
        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                wcss += np.sum((cluster_points - self.centroids[i]) ** 2)
        return wcss

# Read the data
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
response = requests.get(url)
data_whitewine = pd.read_csv(url, delimiter=';')
# Remove the 12th column
data_whitewine.drop(columns=[data_whitewine.columns[11]], inplace=True)

# Convert dataframe to numpy array
X = data_whitewine.values

# Maximum number of clusters
max_clusters = 15

# Perform dynamic k-means clustering and compute WCSS for each number of clusters
wcss_values = []
for n_clusters in range(2, max_clusters + 1):
    custom_kmeans = CustomKMeans(n_clusters=n_clusters)
    labels = custom_kmeans.fit(X)
    wcss_values.append(custom_kmeans.compute_wcss(X, labels))

# Plot Elbow Method graph
plt.figure(figsize=(10, 6))
plt.plot(range(2, max_clusters + 1), wcss_values, marker='o')
plt.title('Elbow Method for Wine Quality Data')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# Perform dynamic k-means clustering for 12 clusters
n_clusters = 12
custom_kmeans = CustomKMeans(n_clusters=n_clusters)
labels = custom_kmeans.fit(X)

# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')
plt.scatter(custom_kmeans.centroids[:, 0], custom_kmeans.centroids[:, 1], marker='x', color='red', label='Centroids')
plt.title('Custom_KMeans Clustering with 12 clusters')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

# Number of centroids
print("Number of centroids:", len(custom_kmeans.centroids))

# Calculate silhouette score
from sklearn.metrics import silhouette_score
silhouette_avg = silhouette_score(X, labels)
print("Silhouette Score:", silhouette_avg)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Read the data
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
data_whitewine = pd.read_csv(url, delimiter=';')
# Remove the 12th column
data_whitewine.drop(columns=[data_whitewine.columns[11]], inplace=True)

# Convert dataframe to numpy array
X = data_whitewine.values

# Maximum number of clusters
max_clusters = 15

# Calculate WCSS for each number of clusters
wcss = []
for i in range(1, max_clusters + 1):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot Elbow Method graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, max_clusters + 1), wcss, marker='o')
plt.title('Elbow Method for Wine Quality Data')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# Fit KMeans with optimal number of clusters
optimal_clusters = 12
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
kmeans.fit(X)

# Get cluster labels and centroids
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Plot clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red', label='Centroids')
plt.title('KMeans Clustering with {} clusters'.format(optimal_clusters))
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

# Calculate silhouette score
silhouette_avg = silhouette_score(X, labels)
print("Silhouette Score:", silhouette_avg)

"""CUSTOME K MEANS PERORMED ON ANOTHER DATA SET

"""

!pip3 install -U ucimlrepo

from ucimlrepo import fetch_ucirepo, list_available_datasets
import ucimlrepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import pprint

# check which datasets can be imported
list_available_datasets()
list_available_datasets(search='Raisin')

Raisin = fetch_ucirepo(id=850)
Raisin.data.original
# Load the original data into a DataFrame
raisin_df = Raisin.data.original
print("Original DataFrame:")
print(raisin_df.head())
# Display the names of all columns
print("Column names in the DataFrame:")
print(raisin_df.columns)
raisin_df.shape

# checking the distribution of Target Varibale
raisin_df['Class'].value_counts()

raisin_df['Class']=raisin_df['Class'].apply(lambda x: 1 if x== 'Kecimen' else 0)
A=raisin_df
A



# Define the CustomKMeans class
class CustomKMeans:
    def __init__(self, n_clusters, max_iter=300):
        self.n_clusters = n_clusters
        self.max_iter = max_iter

    def fit(self, X):
        # Initialize centroids randomly
        self.centroids = self._initialize_centroids(X)

        for _ in range(self.max_iter):
            # Assign each data point to the nearest centroid
            labels = self._assign_clusters(X)

            # Update centroids based on the mean of data points in each cluster
            new_centroids = self._update_centroids(X, labels)

            # Check for convergence
            if np.allclose(self.centroids, new_centroids):
                break

            self.centroids = new_centroids

        return labels

    def _initialize_centroids(self, X):
        return X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]

    def _assign_clusters(self, X):
        distances = np.sqrt(np.sum((X[:, np.newaxis] - self.centroids) ** 2, axis=2))
        return np.argmin(distances, axis=1)

    def _update_centroids(self, X, labels):
        new_centroids = np.zeros_like(self.centroids)

        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                new_centroids[i] = np.mean(cluster_points, axis=0)
            else:
                new_centroids[i] = X[np.random.choice(len(X))]

        return new_centroids

    def compute_wcss(self, X, labels):
        wcss = 0
        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                wcss += np.sum((cluster_points - self.centroids[i]) ** 2)
        return wcss
# Perform dynamic k-means clustering for 12 clusters
n_clusters = 2
custom_kmeans = CustomKMeans(n_clusters=n_clusters)
labels = custom_kmeans.fit(A.values)

# Add cluster column to DataFrame B
A['Cluster'] = labels

A

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming A is your DataFrame and it contains 'Class' and 'Cluster' columns

# Ensure 'Class' and 'Cluster' contain the expected values (0 and 1)
print(A['Class'].unique())
print(A['Cluster'].unique())

# Create confusion matrix
conf_matrix = confusion_matrix(A['Class'], A['Cluster'])

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Kecimen', 'Besni'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix: Class vs Cluster')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Applying Kmeans"""

from ucimlrepo import fetch_ucirepo, list_available_datasets
import ucimlrepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pprint

Raisin = fetch_ucirepo(id=850)
Raisin.data.original
# Load the original data into a DataFrame
raisin_df = Raisin.data.original
print("Original DataFrame:")
print(raisin_df.head())
# Display the names of all columns
print("Column names in the DataFrame:")
print(raisin_df.columns)
raisin_df.shape

raisin_df

from sklearn.cluster  import KMeans
B=raisin_df
B['Class'] = B['Class'].map({'Kecimen':1 ,'Besni':0})

B

# Apply KMeans algorithm
# Convert dataframe to numpy array excluding the 'Class' column for clustering
S = B.drop(columns=['Class']).values
kmeans = KMeans(n_clusters=2, random_state=0)
labels = kmeans.fit_predict(S)

# Add cluster labels to DataFrame B
B['Cluster'] = labels

# Print first 10-20 rows to see the result
print(B.iloc[10:20])

# Create confusion matrix
conf_matrix = confusion_matrix(B['Class'], B['Cluster'])

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Kecimen', 'Besni'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix: Class vs Cluster')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Experimentation on data set 3

"""

from ucimlrepo import fetch_ucirepo, list_available_datasets
import ucimlrepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pprint
list_available_datasets(search='Dry')

"""Fetching and transfoeeming the data"""

Dry_Beans=fetch_ucirepo(id=602)
dx=Dry_Beans.data.original
print(dx)
dx.groupby('Class').size()

"""Drooping the Class colomn

"""

dx['Class'] = dx['Class'].map({'BARBUNYA': 1, 'BOMBAY': 2, 'CALI': 3, 'DERMASON': 4, 'HOROZ': 5, 'SEKER': 6, 'SIRA': 7})

D=dx.iloc[:,0:16]
D

# Define the CustomKMeans class
class CustomKMeans:
    def __init__(self, n_clusters, max_iter=300):
        self.n_clusters = n_clusters
        self.max_iter = max_iter

    def fit(self, X):
        # Initialize centroids randomly
        self.centroids = self._initialize_centroids(X)

        for _ in range(self.max_iter):
            # Assign each data point to the nearest centroid
            labels = self._assign_clusters(X)

            # Update centroids based on the mean of data points in each cluster
            new_centroids = self._update_centroids(X, labels)

            # Check for convergence
            if np.allclose(self.centroids, new_centroids):
                break

            self.centroids = new_centroids

        return labels

    def _initialize_centroids(self, X):
        return X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]

    def _assign_clusters(self, X):
        distances = np.sqrt(np.sum((X[:, np.newaxis] - self.centroids) ** 2, axis=2))
        return np.argmin(distances, axis=1)

    def _update_centroids(self, X, labels):
        new_centroids = np.zeros_like(self.centroids)

        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                new_centroids[i] = np.mean(cluster_points, axis=0)
            else:
                new_centroids[i] = X[np.random.choice(len(X))]

        return new_centroids

    def compute_wcss(self, X, labels):
        wcss = 0
        for i in range(self.n_clusters):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                wcss += np.sum((cluster_points - self.centroids[i]) ** 2)
        return wcss
# Perform dynamic k-means clustering for 12 clusters
n_clusters = 7
custom_kmeans = CustomKMeans(n_clusters=n_clusters)
labels = custom_kmeans.fit(D.values)

D['Cluster']=labels
D

# Assuming dx and d are your DataFrames
# Extract 'Class' column from dx and 'Cluster' column from d
class_column = dx['Class']
cluster_column = D['Cluster']

# Create a new DataFrame
new_df=pd.DataFrame({'Class':class_column,'Cluster':cluster_column})
new_df

from matplotlib import pyplot as plt
import seaborn as sns
new_df.groupby('Class').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
new_df['Cluster'].plot(kind='hist', bins=20, title='Cluster')
plt.gca().spines[['top', 'right',]].set_visible(False)

from sklearn.metrics import confusion_matrix, accuracy_score

# Extract 'Class' and 'Cluster' columns
true_labels = new_df['Class']
predicted_labels = new_df['Cluster']

# Create confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)

# Calculate accuracy score
accuracy = accuracy_score(true_labels, predicted_labels)

print("Confusion Matrix:")
print(conf_matrix)
print("Accuracy Score:", accuracy)

"""Emplotying kmeans on Dry beans"""

from ucimlrepo import fetch_ucirepo, list_available_datasets
import ucimlrepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pprint
list_available_datasets(search='Dry')
Beans=fetch_ucirepo(id=602)
bx=Beans.data.original
print(bx)
bx.groupby('Class').size()
bx['Class'] = bx['Class'].map({'BARBUNYA': 1, 'BOMBAY': 2, 'CALI': 3, 'DERMASON': 4, 'HOROZ': 5, 'SEKER': 6, 'SIRA': 7})
beans=bx.iloc[:,0:16]
beans

# Apply KMeans algorithm
# Convert dataframe to numpy array excluding the 'Class' column for clustering

kmeans = KMeans(n_clusters=7, random_state=0)
labels = kmeans.fit_predict(beans)
# Assuming dx and d are your DataFrames

beans['Cluster']=labels
# Extract 'Class' column from dx and 'Cluster' column from d
class_column1 = bx['Class']
cluster_column1 = beans['Cluster']

# Create a new DataFrame
new_df1=pd.DataFrame({'Class':class_column1,'Cluster':cluster_column1})
new_df1

from sklearn.metrics import confusion_matrix, accuracy_score

# Extract 'Class' and 'Cluster' columns
true_labels = new_df1['Class']
predicted_labels = new_df1['Cluster']

# Create confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)

# Calculate accuracy score
accuracy = accuracy_score(true_labels, predicted_labels)

print("Confusion Matrix:")
print(conf_matrix)
print("Accuracy Score:", accuracy)

"""Conclusion

COMPARISON

DATA-SET 1 WINE_WHITE_QUALITY

Using the Data set wcss mathod Both the mathods have produced approximatimatly same result and we were able to find 12 clusters as optimal number of clusters


Silhouette Score using Custom kmeans for 12 clusters -  0.2976286890457628

Silhouette Score using kmeans for 12 clusters - 0.29302032330526673


From the comparison itself it can be predicted custome kmeans is performing slightly better than the kmeans for the first data set.



DATA-SET-2-"RAISINS"

In the raisins data set the accuracy is checked Using confusion matrix. The comparison is done using class attribute of the dataset. Where two type raisine are categorised based on other attributes such as "Area"	"MajorAxisLength"	"MinorAxisLength"	"Eccentricity"	"ConvexArea"	"Extent"	"Perimeter"


Custome kmeans = Kecimen      Besni

          Kecimen 187         263

          Besni   3           447


 kmeans         = Kecimen      Besni

          Kecimen 187         263

          Besni   3           447

 both models produced exacly same results

Data set 3- "Dry_Beans"   

DATA sets are tested against the Class attributes of the dry beans. In the Data set there are total seven kinds of clases 'types" of dry beans , our model specificals clusters the dtat set nased on the attributes other than class into 7 clusters. The accuracies of the results are checked on the basis of the confusion matrix

custom kmean Accuracy Score: 0.030049224891631768

kmeans Accuracy Score: 0.021673646315480126

From Above results we can see that custome k means has performed ar beeter thann kmeans



"""